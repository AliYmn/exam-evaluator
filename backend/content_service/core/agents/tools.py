"""
LangChain tools for exam evaluation agent
"""

from typing import Dict, Any, List
import time
from langchain_core.tools import tool
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser

from libs.settings import settings
from .models import AnswerKeyOutput, StudentAnswersOutput, EvaluationResult, PerformanceAnalysis, QualityCheckResult


@tool
def parse_answer_key_tool(pdf_text: str) -> Dict[str, Any]:
    """
    Parse answer key PDF and extract questions and answers.

    Args:
        pdf_text: Raw text extracted from PDF

    Returns:
        Dictionary with questions, total_questions, and max_possible_score
    """
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.0-flash-exp",
        google_api_key=settings.GEMINI_API_KEY,
        temperature=0.0,  # ZERO creativity - exact copying only
        max_output_tokens=8192,
    )

    parser = JsonOutputParser(pydantic_object=AnswerKeyOutput)

    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                """You are a precise PDF text extractor. Extract questions and answers EXACTLY as written.

CRITICAL RULES:
- Copy text WORD-FOR-WORD (verbatim)
- Do NOT paraphrase, summarize, or rewrite
- Preserve ALL punctuation and formatting
- Include question numbers exactly as shown

HOW TO SEPARATE:
- question_text: Everything that ASKS (including context, ends with ?)
- expected_answer: The RESPONSE/EXPLANATION (starts after blank line)

{format_instructions}

RETURN ONLY JSON.""",
            ),
            ("user", "Extract the following text VERBATIM (word-for-word):\n\n{pdf_text}"),
        ]
    )

    chain = (
        {"pdf_text": lambda x: x, "format_instructions": lambda _: parser.get_format_instructions()}
        | prompt
        | llm
        | parser
    )

    try:
        print("ðŸ¤– Calling Gemini for answer key parsing...")
        print(f"ðŸ“„ Input text length: {len(pdf_text)} chars")

        # Rate limiting for free tier (10 requests/min)
        time.sleep(7)

        # Clean PDF text to avoid JSON parsing issues
        cleaned_text = pdf_text.replace("\r\n", "\n").replace("\r", "\n")
        # Remove null bytes and other problematic characters
        cleaned_text = cleaned_text.replace("\x00", "").replace("\ufffd", "")

        result = chain.invoke(cleaned_text)

        print(f"âœ… Gemini response received: {result}")

        # Ensure all questions have required fields
        for q in result["questions"]:
            if "max_score" not in q:
                q["max_score"] = 10
            if "keywords" not in q:
                q["keywords"] = []

        # Calculate totals if missing
        if "total_questions" not in result:
            result["total_questions"] = len(result["questions"])
        if "max_possible_score" not in result:
            result["max_possible_score"] = sum(q.get("max_score", 10) for q in result["questions"])

        print(f"ðŸ“Š Final result: {result['total_questions']} questions, {result['max_possible_score']} max score")
        return result
    except Exception as e:
        print(f"âŒ ERROR in parse_answer_key_tool: {str(e)}")
        import traceback

        traceback.print_exc()
        return {"error": str(e), "questions": [], "total_questions": 0, "max_possible_score": 0}


@tool
def parse_student_answer_tool(pdf_text: str, question_count: int) -> List[Dict[str, Any]]:
    """
    Parse student answer sheet and extract student responses.

    Args:
        pdf_text: Raw text extracted from student PDF
        question_count: Expected number of questions

    Returns:
        List of student answers with question numbers
    """
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.0-flash-exp",
        google_api_key=settings.GEMINI_API_KEY,
        temperature=0.0,  # ZERO creativity - exact copying only
        max_output_tokens=8192,
    )

    parser = JsonOutputParser(pydantic_object=StudentAnswersOutput)

    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                """You are a precise student answer extractor. Extract answers EXACTLY as written.

CRITICAL RULES:
- Copy WORD-FOR-WORD (verbatim) including spelling errors
- Do NOT correct grammar or spelling
- Do NOT paraphrase or improve text
- Preserve ALL punctuation and formatting
- If no answer: "[No answer provided]"

EXPECTED QUESTIONS: {question_count}

{format_instructions}

RETURN ONLY JSON.""",
            ),
            ("user", "Extract the student's answers VERBATIM (word-for-word):\n\n{pdf_text}"),
        ]
    )

    chain = (
        {
            "pdf_text": lambda x: x["pdf_text"],
            "question_count": lambda x: x["question_count"],
            "format_instructions": lambda _: parser.get_format_instructions(),
        }
        | prompt
        | llm
        | parser
    )

    try:
        # Rate limiting for free tier (10 requests/min)
        time.sleep(7)

        # Clean PDF text to avoid JSON parsing issues
        cleaned_text = pdf_text.replace("\r\n", "\n").replace("\r", "\n")
        # Remove null bytes and other problematic characters
        cleaned_text = cleaned_text.replace("\x00", "").replace("\ufffd", "")

        result = chain.invoke({"pdf_text": cleaned_text, "question_count": question_count})
        return result.get("answers", [])
    except Exception as e:
        print(f"âŒ ERROR in parse_student_answer_tool: {str(e)}")
        import traceback

        traceback.print_exc()
        return [{"number": i + 1, "student_answer": "[Error parsing]"} for i in range(question_count)]


@tool
def evaluate_answer_tool(
    question_number: int,
    question_text: str,
    expected_answer: str,
    student_answer: str,
    max_score: float,
    keywords: str = "",
) -> Dict[str, Any]:
    """
    Evaluate a single student answer against the expected answer.
    NOW WITH CONFIDENCE SCORE!

    Args:
        question_number: Question number
        question_text: The question text
        expected_answer: Expected answer from answer key
        student_answer: Student's actual answer
        max_score: Maximum score possible
        keywords: Key concepts to look for (comma-separated)

    Returns:
        Dictionary with score, feedback, is_correct, confidence, and reasoning
    """
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.0-flash-exp",
        google_api_key=settings.GEMINI_API_KEY,
        temperature=0.2,
        max_output_tokens=2048,
    )

    parser = JsonOutputParser(pydantic_object=EvaluationResult)

    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                """Sen bir uzman sÄ±nav deÄŸerlendiricisisin. GÃ¶revin Ã¶ÄŸrencinin cevabÄ±nÄ± adil bir ÅŸekilde deÄŸerlendirmektir.

DEÄžERLENDÄ°RME KRÄ°TERLERÄ°:
1. DoÄŸruluk: Cevap beklenen cevapla eÅŸleÅŸiyor mu?
2. TamlÄ±k: TÃ¼m ana noktalar kapsanmÄ±ÅŸ mÄ±?
3. Kesinlik: Bilgiler gerÃ§eklere uygun mu?
4. AÃ§Ä±klÄ±k: Cevap iyi aÃ§Ä±klanmÄ±ÅŸ mÄ±?

PUANLAMA REHBERÄ°:
- %90-100: MÃ¼kemmel, tÃ¼m noktalar doÄŸru ÅŸekilde ele alÄ±nmÄ±ÅŸ
- %70-89: Ä°yi, Ã§oÄŸu nokta kÃ¼Ã§Ã¼k eksiklerle ele alÄ±nmÄ±ÅŸ
- %50-69: Yeterli, bazÄ± anahtar noktalar eksik
- %30-49: KÄ±smi anlayÄ±ÅŸ
- %0-29: YanlÄ±ÅŸ veya yetersiz

GÃœVENÄ°LÄ°RLÄ°K SKORU (confidence):
- 0.9-1.0: Ã‡ok emin (net doÄŸru/yanlÄ±ÅŸ cevap)
- 0.7-0.9: Emin (objektif deÄŸerlendirme mÃ¼mkÃ¼n)
- 0.5-0.7: Orta gÃ¼ven (subjektif unsurlar var)
- 0.0-0.5: DÃ¼ÅŸÃ¼k gÃ¼ven (belirsiz, insan kontrolÃ¼ gerekebilir)

{format_instructions}

ADÄ°L ve YAPICI ol. EÄŸer Ã¶ÄŸrenci cevabÄ± "[No answer provided]" ise, 0 puan ver.
FEEDBACK ve REASONING MUTLAKA TÃœRKÃ‡E OLMALIDIR.""",
            ),
            (
                "user",
                """SORU #{question_number}:
{question_text}

BEKLENÄ°LEN CEVAP (Cevap AnahtarÄ±):
{expected_answer}

Ã–ÄžRENCÄ°NÄ°N CEVABI:
{student_answer}

ARANACAK ANAHTAR KAVRAMLAR: {keywords}
MAKSÄ°MUM PUAN: {max_score}

Ã‡IKTI Ä°Ã‡ERMELÄ°:
- score: Verilen puan
- feedback: TÃ¼rkÃ§e aÃ§Ä±klama
- is_correct: DoÄŸru mu?
- confidence: GÃ¼ven skoru (0-1)
- reasoning: KÄ±sa gerekÃ§e (TÃ¼rkÃ§e)""",
            ),
        ]
    )

    chain = (
        {
            "question_number": lambda x: x["question_number"],
            "question_text": lambda x: x["question_text"],
            "expected_answer": lambda x: x["expected_answer"],
            "student_answer": lambda x: x["student_answer"],
            "keywords": lambda x: x["keywords"],
            "max_score": lambda x: x["max_score"],
            "format_instructions": lambda _: parser.get_format_instructions(),
        }
        | prompt
        | llm
        | parser
    )

    # Retry logic with exponential backoff for rate limits
    max_retries = 3
    base_delay = 7  # Free tier: 10 requests/min = 6 seconds + buffer

    for attempt in range(max_retries):
        try:
            # Rate limiting: Wait between requests to avoid quota exceeded
            if attempt > 0:
                wait_time = base_delay * (2**attempt)  # Exponential backoff
                print(f"â³ Rate limit retry {attempt}/{max_retries}, waiting {wait_time}s...")
                time.sleep(wait_time)
            else:
                # Always wait to respect free tier limits (10 req/min)
                time.sleep(base_delay)

            # Clean text inputs to avoid JSON parsing issues
            def clean_text(text):
                if not isinstance(text, str):
                    return text
                return text.replace("\r\n", "\n").replace("\r", "\n").replace("\x00", "").replace("\ufffd", "")

            input_data = {
                "question_number": question_number,
                "question_text": clean_text(question_text),
                "expected_answer": clean_text(expected_answer),
                "student_answer": clean_text(student_answer),
                "keywords": keywords,
                "max_score": max_score,
            }
            result = chain.invoke(input_data)

            # Ensure score is within bounds
            result["score"] = min(max(result["score"], 0), max_score)

            # Ensure required fields
            if "is_correct" not in result:
                result["is_correct"] = result["score"] >= (max_score * 0.7)
            if "confidence" not in result:
                result["confidence"] = 0.8  # Default confidence
            if "reasoning" not in result:
                result["reasoning"] = "Standart deÄŸerlendirme"

            return result

        except Exception as e:
            error_msg = str(e)

            # Check if it's a rate limit error
            if "429" in error_msg or "quota" in error_msg.lower():
                if attempt < max_retries - 1:
                    print(f"âš ï¸ Rate limit hit (attempt {attempt + 1}/{max_retries})")
                    continue  # Retry with exponential backoff
                else:
                    print(f"âŒ Rate limit exceeded after {max_retries} attempts")

            # If not rate limit or final attempt, return error
            return {
                "score": 0,
                "feedback": "DeÄŸerlendirme hatasÄ±: API limiti aÅŸÄ±ldÄ±. LÃ¼tfen birkaÃ§ dakika bekleyin veya API planÄ±nÄ±zÄ± yÃ¼kseltin.",
                "is_correct": False,
                "confidence": 0.0,
                "reasoning": "API rate limit",
            }

    # Fallback (should never reach here)
    return {
        "score": 0,
        "feedback": "DeÄŸerlendirme tamamlanamadÄ±",
        "is_correct": False,
        "confidence": 0.0,
        "reasoning": "Bilinmeyen hata",
    }


@tool
def quality_check_tool(evaluation_data: Dict[str, Any], max_score: float) -> Dict[str, Any]:
    """
    NEW TOOL: Quality check / self-correction for evaluations.
    Reviews the evaluation to ensure it's fair and accurate.

    Args:
        evaluation_data: The evaluation result to check
        max_score: Maximum possible score

    Returns:
        Quality check result with is_acceptable, issues, and suggested_corrections
    """
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.0-flash-exp",
        google_api_key=settings.GEMINI_API_KEY,
        temperature=0.1,
        max_output_tokens=1024,
    )

    parser = JsonOutputParser(pydantic_object=QualityCheckResult)

    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                """Sen bir kalite kontrol uzmanÄ±sÄ±n. GÃ¶revin sÄ±nav deÄŸerlendirmelerinin adil ve tutarlÄ± olup olmadÄ±ÄŸÄ±nÄ± kontrol etmek.

KONTROL KRÄ°TERLERÄ°:
1. Puan feedback ile uyumlu mu?
2. Puan aralÄ±ÄŸÄ± mantÄ±klÄ± mÄ±? (0 ile max_score arasÄ±)
3. Feedback yeterince aÃ§Ä±klayÄ±cÄ± mÄ±?
4. Puanlama rehberine uyuluyor mu?

KABUL EDÄ°LEBÄ°LÄ°R DEÄžÄ°L ise issues listesinde belirt.

{format_instructions}""",
            ),
            (
                "user",
                """DEÄžERLENDÄ°RME KONTROL:

Verilen Puan: {score}/{max_score}
Feedback: {feedback}
Confidence: {confidence}
Reasoning: {reasoning}

Bu deÄŸerlendirme kaliteli ve adil mi?""",
            ),
        ]
    )

    chain = (
        {
            "score": lambda x: x["score"],
            "max_score": lambda x: x["max_score"],
            "feedback": lambda x: x["feedback"],
            "confidence": lambda x: x.get("confidence", 0.8),
            "reasoning": lambda x: x.get("reasoning", "Yok"),
            "format_instructions": lambda _: parser.get_format_instructions(),
        }
        | prompt
        | llm
        | parser
    )

    try:
        input_data = {
            "score": evaluation_data.get("score", 0),
            "max_score": max_score,
            "feedback": evaluation_data.get("feedback", ""),
            "confidence": evaluation_data.get("confidence", 0.8),
            "reasoning": evaluation_data.get("reasoning", "Yok"),
        }
        result = chain.invoke(input_data)

        return result
    except Exception:
        return {
            "is_acceptable": True,  # Default to acceptable if check fails
            "issues": [],
            "suggested_corrections": None,
            "confidence": 0.5,
        }


@tool
def analyze_performance_tool(
    student_name: str, total_score: float, max_score: float, percentage: float, questions_summary: str
) -> Dict[str, Any]:
    """
    Analyze overall student performance and identify strengths/weaknesses.
    NOW WITH CONFIDENCE!

    Args:
        student_name: Student's name
        total_score: Total score achieved
        max_score: Maximum possible score
        percentage: Percentage score
        questions_summary: Summary of all question evaluations

    Returns:
        Dictionary with strengths, weaknesses, and confidence
    """
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.0-flash-exp",
        google_api_key=settings.GEMINI_API_KEY,
        temperature=0.3,
        max_output_tokens=2048,
    )

    parser = JsonOutputParser(pydantic_object=PerformanceAnalysis)

    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                """Sen bir eÄŸitim analistisin. Ã–ÄŸrencinin sÄ±nav performansÄ±nÄ± analiz edip gÃ¼Ã§lÃ¼/zayÄ±f yÃ¶nlerini belirle.

Ã–NEMLÄ° KURALLAR:
- Her liste iÃ§in 2-4 madde yaz
- KÄ±sa ve net cÃ¼mleler kullan (maksimum 10-15 kelime)
- TÃ¼rkÃ§e yaz
- Spesifik ol (Ã¶rneÄŸin: "Genel olarak iyi" deÄŸil, "Tarihsel olaylarÄ± kronolojik sÄ±raya koyuyor")
- confidence: Analizine ne kadar gÃ¼veniyorsun? (0-1)

{format_instructions}""",
            ),
            (
                "user",
                """Ã–ÄžRENCÄ° ANALÄ°ZÄ°:
Ã–ÄŸrenci: {student_name}
Toplam Puan: {total_score}/{max_score} (%{percentage})

SORULAR VE CEVAPLAR:
{questions_summary}

GÃ–REV:
YukarÄ±daki sÄ±nav performansÄ±nÄ± analiz ederek Ã¶ÄŸrencinin:
1. GÃœÃ‡LÃœ YÃ–NLERÄ°NÄ° (strengths) - Ne yapÄ±yor iyi? Hangi becerileri gÃ¼Ã§lÃ¼?
2. ZAYIF YÃ–NLERÄ°NÄ° (weaknesses) - Nerelerde zorlanÄ±yor? Hangi eksiklikleri var?
3. CONFIDENCE - Analizine ne kadar gÃ¼veniyorsun?

belirle.""",
            ),
        ]
    )

    chain = (
        {
            "student_name": lambda x: x["student_name"],
            "total_score": lambda x: x["total_score"],
            "max_score": lambda x: x["max_score"],
            "percentage": lambda x: x["percentage"],
            "questions_summary": lambda x: x["questions_summary"],
            "format_instructions": lambda _: parser.get_format_instructions(),
        }
        | prompt
        | llm
        | parser
    )

    try:
        # Rate limiting for free tier (10 requests/min)
        time.sleep(7)

        result = chain.invoke(
            {
                "student_name": student_name,
                "total_score": total_score,
                "max_score": max_score,
                "percentage": percentage,
                "questions_summary": questions_summary,
            }
        )

        # Validate structure
        if not isinstance(result.get("strengths"), list):
            result["strengths"] = []
        if not isinstance(result.get("weaknesses"), list):
            result["weaknesses"] = []
        if "confidence" not in result:
            result["confidence"] = 0.8

        return result
    except Exception:
        return {
            "strengths": ["BazÄ± sorulara doÄŸru yanÄ±t verdi"],
            "weaknesses": ["Genel performans dÃ¼ÅŸÃ¼k, daha fazla Ã§alÄ±ÅŸma gerekiyor"],
            "confidence": 0.5,
        }
