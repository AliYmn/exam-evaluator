"""
LangChain tools for exam evaluation agent
"""

from typing import Dict, Any, List
import time
from langchain_core.tools import tool
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser

from libs.settings import settings
from .models import AnswerKeyOutput, StudentAnswersOutput, EvaluationResult, PerformanceAnalysis, QualityCheckResult


@tool
def parse_answer_key_tool(pdf_text: str) -> Dict[str, Any]:
    """
    Parse answer key PDF and extract questions and answers.

    Args:
        pdf_text: Raw text extracted from PDF

    Returns:
        Dictionary with questions, total_questions, and max_possible_score
    """
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.0-flash-exp",
        google_api_key=settings.GEMINI_API_KEY,
        temperature=0.0,  # ZERO creativity - exact copying only
        max_output_tokens=8192,
    )

    parser = JsonOutputParser(pydantic_object=AnswerKeyOutput)

    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                """You are a precise PDF text extractor. Extract questions and answers EXACTLY as written.

CRITICAL RULES:
- Copy text WORD-FOR-WORD (verbatim)
- Do NOT paraphrase, summarize, or rewrite
- Preserve ALL punctuation and formatting
- Include question numbers exactly as shown

HOW TO SEPARATE:
- question_text: Everything that ASKS (including context, ends with ?)
- expected_answer: The RESPONSE/EXPLANATION (starts after blank line)

{format_instructions}

RETURN ONLY JSON.""",
            ),
            ("user", "Extract the following text VERBATIM (word-for-word):\n\n{pdf_text}"),
        ]
    )

    chain = (
        {"pdf_text": lambda x: x, "format_instructions": lambda _: parser.get_format_instructions()}
        | prompt
        | llm
        | parser
    )

    try:
        print("ü§ñ Calling Gemini for answer key parsing...")
        print(f"üìÑ Input text length: {len(pdf_text)} chars")

        # Rate limiting for free tier (10 requests/min)
        time.sleep(7)

        # Clean PDF text to avoid JSON parsing issues
        cleaned_text = pdf_text.replace("\r\n", "\n").replace("\r", "\n")
        # Remove null bytes and other problematic characters
        cleaned_text = cleaned_text.replace("\x00", "").replace("\ufffd", "")

        result = chain.invoke(cleaned_text)

        print(f"‚úÖ Gemini response received: {result}")

        # Ensure all questions have required fields
        for q in result["questions"]:
            if "max_score" not in q:
                q["max_score"] = 10
            if "keywords" not in q:
                q["keywords"] = []

        # Calculate totals if missing
        if "total_questions" not in result:
            result["total_questions"] = len(result["questions"])
        if "max_possible_score" not in result:
            result["max_possible_score"] = sum(q.get("max_score", 10) for q in result["questions"])

        print(f"üìä Final result: {result['total_questions']} questions, {result['max_possible_score']} max score")
        return result
    except Exception as e:
        print(f"‚ùå ERROR in parse_answer_key_tool: {str(e)}")
        import traceback

        traceback.print_exc()
        return {"error": str(e), "questions": [], "total_questions": 0, "max_possible_score": 0}


@tool
def parse_student_answer_tool(pdf_text: str, question_count: int) -> List[Dict[str, Any]]:
    """
    Parse student answer sheet and extract student responses.

    Args:
        pdf_text: Raw text extracted from student PDF
        question_count: Expected number of questions

    Returns:
        List of student answers with question numbers
    """
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.0-flash-exp",
        google_api_key=settings.GEMINI_API_KEY,
        temperature=0.0,  # ZERO creativity - exact copying only
        max_output_tokens=8192,
    )

    parser = JsonOutputParser(pydantic_object=StudentAnswersOutput)

    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                """You are a precise student answer extractor. Extract answers EXACTLY as written.

CRITICAL RULES:
- Copy WORD-FOR-WORD (verbatim) including spelling errors
- Do NOT correct grammar or spelling
- Do NOT paraphrase or improve text
- Preserve ALL punctuation and formatting
- If no answer: "[No answer provided]"

EXPECTED QUESTIONS: {question_count}

{format_instructions}

RETURN ONLY JSON.""",
            ),
            ("user", "Extract the student's answers VERBATIM (word-for-word):\n\n{pdf_text}"),
        ]
    )

    chain = (
        {
            "pdf_text": lambda x: x["pdf_text"],
            "question_count": lambda x: x["question_count"],
            "format_instructions": lambda _: parser.get_format_instructions(),
        }
        | prompt
        | llm
        | parser
    )

    try:
        # Rate limiting for free tier (10 requests/min)
        time.sleep(7)

        # Clean PDF text to avoid JSON parsing issues
        cleaned_text = pdf_text.replace("\r\n", "\n").replace("\r", "\n")
        # Remove null bytes and other problematic characters
        cleaned_text = cleaned_text.replace("\x00", "").replace("\ufffd", "")

        result = chain.invoke({"pdf_text": cleaned_text, "question_count": question_count})
        return result.get("answers", [])
    except Exception as e:
        print(f"‚ùå ERROR in parse_student_answer_tool: {str(e)}")
        import traceback

        traceback.print_exc()
        return [{"number": i + 1, "student_answer": "[Error parsing]"} for i in range(question_count)]


@tool
def evaluate_answer_tool(
    question_number: int,
    question_text: str,
    expected_answer: str,
    student_answer: str,
    max_score: float,
    keywords: str = "",
) -> Dict[str, Any]:
    """
    Evaluate a single student answer against the expected answer.
    NOW WITH CONFIDENCE SCORE!

    Args:
        question_number: Question number
        question_text: The question text
        expected_answer: Expected answer from answer key
        student_answer: Student's actual answer
        max_score: Maximum score possible
        keywords: Key concepts to look for (comma-separated)

    Returns:
        Dictionary with score, feedback, is_correct, confidence, and reasoning
    """
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.0-flash-exp",
        google_api_key=settings.GEMINI_API_KEY,
        temperature=0.2,
        max_output_tokens=2048,
    )

    parser = JsonOutputParser(pydantic_object=EvaluationResult)

    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                """Sen bir uzman sƒ±nav deƒüerlendiricisisin. G√∂revin √∂ƒürencinin cevabƒ±nƒ± adil bir ≈üekilde deƒüerlendirmektir.

DEƒûERLENDƒ∞RME KRƒ∞TERLERƒ∞:
1. Doƒüruluk: Cevap beklenen cevapla e≈üle≈üiyor mu?
2. Tamlƒ±k: T√ºm ana noktalar kapsanmƒ±≈ü mƒ±?
3. Kesinlik: Bilgiler ger√ßeklere uygun mu?
4. A√ßƒ±klƒ±k: Cevap iyi a√ßƒ±klanmƒ±≈ü mƒ±?

PUANLAMA REHBERƒ∞:
- %90-100: M√ºkemmel, t√ºm noktalar doƒüru ≈üekilde ele alƒ±nmƒ±≈ü
- %70-89: ƒ∞yi, √ßoƒüu nokta k√º√ß√ºk eksiklerle ele alƒ±nmƒ±≈ü
- %50-69: Yeterli, bazƒ± anahtar noktalar eksik
- %30-49: Kƒ±smi anlayƒ±≈ü
- %0-29: Yanlƒ±≈ü veya yetersiz

G√úVENƒ∞Lƒ∞RLƒ∞K SKORU (confidence):
- 0.9-1.0: √áok emin (net doƒüru/yanlƒ±≈ü cevap)
- 0.7-0.9: Emin (objektif deƒüerlendirme m√ºmk√ºn)
- 0.5-0.7: Orta g√ºven (subjektif unsurlar var)
- 0.0-0.5: D√º≈ü√ºk g√ºven (belirsiz, insan kontrol√º gerekebilir)

{format_instructions}

ADƒ∞L ve YAPICI ol. Eƒüer √∂ƒürenci cevabƒ± "[No answer provided]" ise, 0 puan ver.
FEEDBACK ve REASONING MUTLAKA T√úRK√áE OLMALIDIR.""",
            ),
            (
                "user",
                """SORU #{question_number}:
{question_text}

BEKLENƒ∞LEN CEVAP (Cevap Anahtarƒ±):
{expected_answer}

√ñƒûRENCƒ∞Nƒ∞N CEVABI:
{student_answer}

ARANACAK ANAHTAR KAVRAMLAR: {keywords}
MAKSƒ∞MUM PUAN: {max_score}

√áIKTI ƒ∞√áERMELƒ∞:
- score: Verilen puan
- feedback: T√ºrk√ße a√ßƒ±klama
- is_correct: Doƒüru mu?
- confidence: G√ºven skoru (0-1)
- reasoning: Kƒ±sa gerek√ße (T√ºrk√ße)""",
            ),
        ]
    )

    chain = (
        {
            "question_number": lambda x: x["question_number"],
            "question_text": lambda x: x["question_text"],
            "expected_answer": lambda x: x["expected_answer"],
            "student_answer": lambda x: x["student_answer"],
            "keywords": lambda x: x["keywords"],
            "max_score": lambda x: x["max_score"],
            "format_instructions": lambda _: parser.get_format_instructions(),
        }
        | prompt
        | llm
        | parser
    )

    # Retry logic with exponential backoff for rate limits
    max_retries = 3
    base_delay = 7  # Free tier: 10 requests/min = 6 seconds + buffer

    for attempt in range(max_retries):
        try:
            # Rate limiting: Wait between requests to avoid quota exceeded
            if attempt > 0:
                wait_time = base_delay * (2**attempt)  # Exponential backoff
                print(f"‚è≥ Rate limit retry {attempt}/{max_retries}, waiting {wait_time}s...")
                time.sleep(wait_time)
            else:
                # Always wait to respect free tier limits (10 req/min)
                time.sleep(base_delay)

            # Clean text inputs to avoid JSON parsing issues
            def clean_text(text):
                if not isinstance(text, str):
                    return text
                return text.replace("\r\n", "\n").replace("\r", "\n").replace("\x00", "").replace("\ufffd", "")

            input_data = {
                "question_number": question_number,
                "question_text": clean_text(question_text),
                "expected_answer": clean_text(expected_answer),
                "student_answer": clean_text(student_answer),
                "keywords": keywords,
                "max_score": max_score,
            }
            result = chain.invoke(input_data)

            # Ensure score is within bounds
            result["score"] = min(max(result["score"], 0), max_score)

            # Ensure required fields
            if "is_correct" not in result:
                result["is_correct"] = result["score"] >= (max_score * 0.7)
            if "confidence" not in result:
                result["confidence"] = 0.8  # Default confidence
            if "reasoning" not in result:
                result["reasoning"] = "Standart deƒüerlendirme"

            return result

        except Exception as e:
            error_msg = str(e)

            # Check if it's a rate limit error
            if "429" in error_msg or "quota" in error_msg.lower():
                if attempt < max_retries - 1:
                    print(f"‚ö†Ô∏è Rate limit hit (attempt {attempt + 1}/{max_retries})")
                    continue  # Retry with exponential backoff
                else:
                    print(f"‚ùå Rate limit exceeded after {max_retries} attempts")

            # If not rate limit or final attempt, return error
            return {
                "score": 0,
                "feedback": "Deƒüerlendirme hatasƒ±: API limiti a≈üƒ±ldƒ±. L√ºtfen birka√ß dakika bekleyin veya API planƒ±nƒ±zƒ± y√ºkseltin.",
                "is_correct": False,
                "confidence": 0.0,
                "reasoning": "API rate limit",
            }

    # Fallback (should never reach here)
    return {
        "score": 0,
        "feedback": "Deƒüerlendirme tamamlanamadƒ±",
        "is_correct": False,
        "confidence": 0.0,
        "reasoning": "Bilinmeyen hata",
    }


@tool
def quality_check_tool(evaluation_data: Dict[str, Any], max_score: float) -> Dict[str, Any]:
    """
    NEW TOOL: Quality check / self-correction for evaluations.
    Reviews the evaluation to ensure it's fair and accurate.

    Args:
        evaluation_data: The evaluation result to check
        max_score: Maximum possible score

    Returns:
        Quality check result with is_acceptable, issues, and suggested_corrections
    """
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.0-flash-exp",
        google_api_key=settings.GEMINI_API_KEY,
        temperature=0.1,
        max_output_tokens=1024,
    )

    parser = JsonOutputParser(pydantic_object=QualityCheckResult)

    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                """Sen bir kalite kontrol uzmanƒ±sƒ±n. G√∂revin sƒ±nav deƒüerlendirmelerinin adil ve tutarlƒ± olup olmadƒ±ƒüƒ±nƒ± kontrol etmek.

KONTROL KRƒ∞TERLERƒ∞:
1. Puan feedback ile uyumlu mu?
2. Puan aralƒ±ƒüƒ± mantƒ±klƒ± mƒ±? (0 ile max_score arasƒ±)
3. Feedback yeterince a√ßƒ±klayƒ±cƒ± mƒ±?
4. Puanlama rehberine uyuluyor mu?

KABUL EDƒ∞LEBƒ∞Lƒ∞R DEƒûƒ∞L ise issues listesinde belirt.

{format_instructions}""",
            ),
            (
                "user",
                """DEƒûERLENDƒ∞RME KONTROL:

Verilen Puan: {score}/{max_score}
Feedback: {feedback}
Confidence: {confidence}
Reasoning: {reasoning}

Bu deƒüerlendirme kaliteli ve adil mi?""",
            ),
        ]
    )

    chain = (
        {
            "score": lambda x: x["score"],
            "max_score": lambda x: x["max_score"],
            "feedback": lambda x: x["feedback"],
            "confidence": lambda x: x.get("confidence", 0.8),
            "reasoning": lambda x: x.get("reasoning", "Yok"),
            "format_instructions": lambda _: parser.get_format_instructions(),
        }
        | prompt
        | llm
        | parser
    )

    try:
        input_data = {
            "score": evaluation_data.get("score", 0),
            "max_score": max_score,
            "feedback": evaluation_data.get("feedback", ""),
            "confidence": evaluation_data.get("confidence", 0.8),
            "reasoning": evaluation_data.get("reasoning", "Yok"),
        }
        result = chain.invoke(input_data)

        return result
    except Exception:
        return {
            "is_acceptable": True,  # Default to acceptable if check fails
            "issues": [],
            "suggested_corrections": None,
            "confidence": 0.5,
        }


@tool
def analyze_performance_tool(
    student_name: str, total_score: float, max_score: float, percentage: float, questions_summary: str
) -> Dict[str, Any]:
    """
    Analyze overall student performance and identify strengths/weaknesses.
    NOW WITH CONFIDENCE!

    Args:
        student_name: Student's name
        total_score: Total score achieved
        max_score: Maximum possible score
        percentage: Percentage score
        questions_summary: Summary of all question evaluations

    Returns:
        Dictionary with strengths, weaknesses, and confidence
    """
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.0-flash-exp",
        google_api_key=settings.GEMINI_API_KEY,
        temperature=0.3,
        max_output_tokens=2048,
    )

    parser = JsonOutputParser(pydantic_object=PerformanceAnalysis)

    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                """Sen bir eƒüitim analistisin. √ñƒürencinin sƒ±nav performansƒ±nƒ± analiz edip g√º√ßl√º/zayƒ±f y√∂nlerini belirle.

√ñNEMLƒ∞ KURALLAR:
- Her liste i√ßin 2-4 madde yaz
- Kƒ±sa ve net c√ºmleler kullan (maksimum 10-15 kelime)
- T√ºrk√ße yaz
- Spesifik ol (√∂rneƒüin: "Genel olarak iyi" deƒüil, "Tarihsel olaylarƒ± kronolojik sƒ±raya koyuyor")
- confidence: Analizine ne kadar g√ºveniyorsun? (0-1)

{format_instructions}""",
            ),
            (
                "user",
                """√ñƒûRENCƒ∞ ANALƒ∞Zƒ∞:
√ñƒürenci: {student_name}
Toplam Puan: {total_score}/{max_score} (%{percentage})

SORULAR VE CEVAPLAR:
{questions_summary}

G√ñREV:
Yukarƒ±daki sƒ±nav performansƒ±nƒ± analiz ederek √∂ƒürencinin:
1. G√ú√áL√ú Y√ñNLERƒ∞Nƒ∞ (strengths) - Ne yapƒ±yor iyi? Hangi becerileri g√º√ßl√º?
2. ZAYIF Y√ñNLERƒ∞Nƒ∞ (weaknesses) - Nerelerde zorlanƒ±yor? Hangi eksiklikleri var?
3. CONFIDENCE - Analizine ne kadar g√ºveniyorsun?

belirle.""",
            ),
        ]
    )

    chain = (
        {
            "student_name": lambda x: x["student_name"],
            "total_score": lambda x: x["total_score"],
            "max_score": lambda x: x["max_score"],
            "percentage": lambda x: x["percentage"],
            "questions_summary": lambda x: x["questions_summary"],
            "format_instructions": lambda _: parser.get_format_instructions(),
        }
        | prompt
        | llm
        | parser
    )

    try:
        # Rate limiting for free tier (10 requests/min)
        time.sleep(7)

        result = chain.invoke(
            {
                "student_name": student_name,
                "total_score": total_score,
                "max_score": max_score,
                "percentage": percentage,
                "questions_summary": questions_summary,
            }
        )

        # Validate structure
        if not isinstance(result.get("strengths"), list):
            result["strengths"] = []
        if not isinstance(result.get("weaknesses"), list):
            result["weaknesses"] = []
        if "confidence" not in result:
            result["confidence"] = 0.8

        return result
    except Exception:
        return {
            "strengths": ["Bazƒ± sorulara doƒüru yanƒ±t verdi"],
            "weaknesses": ["Genel performans d√º≈ü√ºk, daha fazla √ßalƒ±≈üma gerekiyor"],
            "confidence": 0.5,
        }
